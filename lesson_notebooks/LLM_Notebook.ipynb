{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM APIs - HAVE A SEPARATE BRANCH TO RUN ALL OF THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to investigate the metrics we will be using. We can do this and take an initial look at the data for this section - translating French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olinewcombe/Documents/GitHub/LLMWorkshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/translation/en-fr-filtered.csv')\n",
    "df = pd.read_csv('data/translation/english_french_filtered.csv', index_col=0)\n",
    "testing_df = df[:50] # This is what we will be using for the majority of our testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below loads in the model we will be using, it may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 52038.51it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/olinewcombe/Documents/GitHub/LLMWorkshop/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": df.en.iloc[0], # Source text - this is what is to be translated\n",
    "        \"mt\": df.fr.iloc[0], # Machine translation - this is what has been translated by our AI\n",
    "        \"ref\": df.fr.iloc[0] # Reference text - this is the ground truth translation, or 'gold standard', what we are assuming is perfect.\n",
    "    },\n",
    "    {\n",
    "        \"src\": df.en.iloc[0],\n",
    "        \"mt\": \"Hiii girlie\",\n",
    "        \"ref\": \"Hiii girlie\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model_output = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9269278645515442, 0.9839427471160889]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the second score is higher - why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Play around with examples in languages you know, how good is it (genuine question)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now move onto the main event -  the OpenAI API! You should have set up the API key as in the email sent out previously, if not, find the API key\n",
    "\n",
    "We can do a quick test to see if you are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "# client = OpenAI(api_key=\"sk-xxxxxxxxxxxxxxxx\") # This is a stopgap solution, do not use this in production!!\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent lines of code,  \n",
      "Dreams woven in algorithms,  \n",
      "Life in bits and bytes.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the structure of this. We initialise a client object, which allows us to access the OpenAI API and make requests via it. A method of this client is to create a completion, that is ask the LLM to complete a response from our prompt. The client has many other options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_base_url',\n",
       " '_build_headers',\n",
       " '_build_request',\n",
       " '_calculate_retry_timeout',\n",
       " '_client',\n",
       " '_custom_headers',\n",
       " '_custom_query',\n",
       " '_default_stream_cls',\n",
       " '_enforce_trailing_slash',\n",
       " '_idempotency_header',\n",
       " '_idempotency_key',\n",
       " '_limits',\n",
       " '_make_sse_decoder',\n",
       " '_make_status_error',\n",
       " '_make_status_error_from_response',\n",
       " '_maybe_override_cast_to',\n",
       " '_parse_retry_after_header',\n",
       " '_platform',\n",
       " '_prepare_options',\n",
       " '_prepare_request',\n",
       " '_prepare_url',\n",
       " '_process_response',\n",
       " '_process_response_data',\n",
       " '_proxies',\n",
       " '_request',\n",
       " '_request_api_list',\n",
       " '_retry_request',\n",
       " '_serialize_multipartform',\n",
       " '_should_retry',\n",
       " '_should_stream_response_body',\n",
       " '_strict_response_validation',\n",
       " '_transport',\n",
       " '_validate_headers',\n",
       " '_version',\n",
       " 'api_key',\n",
       " 'audio',\n",
       " 'auth_headers',\n",
       " 'base_url',\n",
       " 'batches',\n",
       " 'beta',\n",
       " 'chat',\n",
       " 'close',\n",
       " 'completions',\n",
       " 'copy',\n",
       " 'custom_auth',\n",
       " 'default_headers',\n",
       " 'default_query',\n",
       " 'delete',\n",
       " 'embeddings',\n",
       " 'files',\n",
       " 'fine_tuning',\n",
       " 'get',\n",
       " 'get_api_list',\n",
       " 'images',\n",
       " 'is_closed',\n",
       " 'max_retries',\n",
       " 'models',\n",
       " 'moderations',\n",
       " 'organization',\n",
       " 'patch',\n",
       " 'platform_headers',\n",
       " 'post',\n",
       " 'project',\n",
       " 'put',\n",
       " 'qs',\n",
       " 'request',\n",
       " 'timeout',\n",
       " 'uploads',\n",
       " 'user_agent',\n",
       " 'websocket_base_url',\n",
       " 'with_options',\n",
       " 'with_raw_response',\n",
       " 'with_streaming_response']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify the model along with our message to the API. This is familiar, our message 'content' here is exactly the sort of thing we may ask ChatGPT. But, we have the extra argument of 'role' - note that messages is a list! Here we are only utilising the 'user' role, so it is in essence what one would do on ChatGPT.com. There is more flexibility here however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a dog and can only answer with 'woof'\"}, # Technically the 'system' prompt for OpenAI is now called 'developer'.\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof, woof, woof, woof,  \n",
      "Whispers of code and data,  \n",
      "Woof, woof, dreams emerge.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see this has the potential to have a lot of impact and adds an extra level of complexity to prompt design. It can be shown that by simply telling the LLM it is an expert in something, or giving it increased contextual clues even if they are not directly relevant to the task, can improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of role is that of the assistant. It can also be used to show the model what to do, but isn't so forceful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"woof\"},\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about data science\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"woof\"},\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about machine learning\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"woof\"},\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about deep learning\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers intertwine,  \n",
      "Neurons spark with thought and dreams,  \n",
      "Data whispers truth.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates using the assistant messages has more of a subtle effect. It is 'show' not 'tell' - the prior beliefs of gpt4o-mini as to what a good response is in this case are overriding our examples. LLMs are probabilistic models - this is almost Bayesian. We shall quantify the effect of this <b>few-shot prompting</b> later - it is very effective in the right use cases, this one is rather forced.\n",
    "\n",
    "These methods so far are tinkering with the prompt via natural language. This is hard to quantify - how do you test different prompts in a principled way? Some parameters that are easier to visualise are temperature and top p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature defaults to 1 from [0,2]. It is a measure of how 'random' our output will be, 0 represents close to deterministic behaviour, whereas 2 is more creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Repeat the same prompts with varying temperature and top p to assess the effect they have. Only vary one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "prompt = ''\n",
    "temperature = 1\n",
    "top_p = 1\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt} # Throughout please try to keep token utilisation low\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Question: For our use case, translation, what sort of temperature/top p should we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto getting a baseline for our dataset. We can do this simply by looping over.\n",
    "\n",
    "## Exercise \n",
    "\n",
    "1) Use BasicAPICall() from llm_utils to write a function that loops over our dataset, translates the <b>French text into English</b>, and stores the results in a dataframe. Below is a structure for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =''\n",
    "\n",
    "def BaselineEval(df, system_prompt):\n",
    "    # Setup output structure here\n",
    "    for en, fr in zip(df.en, df.fr):\n",
    "        # Loop logic here including BasicAPICall() and adding to output structure in a way that makes it easy to process\n",
    "        # f strings or .format() could be useful here for the prompt\n",
    "        pass\n",
    "    new_df = '' # I suggest putting this into a dataframe, potentially combined with the original df for ease. Not essential, look at pd.concat\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaselineEval(df, system_prompt, model=\"gpt-4o-mini\",):\n",
    "    output_list = []\n",
    "    for en, fr in zip(df.en, df.fr):\n",
    "        prompt = f\"Translate the following French text into English: {fr}\"\n",
    "        output_text = BasicAPICall(prompt, model = model, system_prompt=system_prompt)\n",
    "        output_list.append(output_text)\n",
    "    new_df = pd.concat([df, pd.Series(output_list, name = 'translation')], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert in language translation, especially translating French into English. \n",
    "                    Reply with the only translated text, with no other information.\"\"\"\n",
    "evals_df = BaselineEval(testing_df, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Write functions to, given this new df, compute COMET scores for each translation. Add this as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done either individually, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleComet(src, mt, ref):\n",
    "    data = [\n",
    "        {\n",
    "            \"src\": src, # Source text - this is what is to be translated\n",
    "            \"mt\": mt, # Machine translation - this is what has been translated by our AI\n",
    "            \"ref\": ref # Reference text - this is the ground truth translation, or 'gold standard', what we are assuming is perfect.\n",
    "        }\n",
    "    ]\n",
    "    model_output = model.predict(data)\n",
    "    return model_output.scores[0]\n",
    "\n",
    "def DataframeComet(df):\n",
    "    scores = []\n",
    "    for en, fr, translation in zip(df.en, df.fr, df.translation):\n",
    "        scores.append(SingleComet(fr, translation, en))\n",
    "    new_df = pd.concat([df, pd.Series(scores, name = 'scores')], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or (Ask ChatGPT to) use batch processing for performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleCometBatch(data):\n",
    "    \"\"\"\n",
    "    Process a batch of data through the COMET model.\n",
    "    Args:\n",
    "        data (list of dicts): Each dict contains 'src', 'mt', and 'ref' keys.\n",
    "    Returns:\n",
    "        list of float: COMET scores for each input in the batch.\n",
    "    \"\"\"\n",
    "    model_output = model.predict(data)\n",
    "    return model_output.scores\n",
    "\n",
    "def DataframeCometBatch(df, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process a dataframe with COMET scoring using batching for efficiency.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'en', 'fr', and 'translation' columns.\n",
    "        batch_size (int): Number of examples to process in a single batch.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'scores' column.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\n",
    "            \"src\": fr,  # Source text (fr)\n",
    "            \"mt\": translation,  # Machine translation\n",
    "            \"ref\": en  # Reference text (en)\n",
    "        }\n",
    "        for en, fr, translation in zip(df.en, df.fr, df.translation)\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        scores.extend(SingleCometBatch(batch))\n",
    "    \n",
    "    # Add the scores as a new column to the DataFrame\n",
    "    df['scores'] = scores\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "scores_df = DataframeCometBatch(evals_df, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is why we call upon the Commission and the Council to support this process by joining us in making representations to the Tunisian authorities.',\n",
       " 'That is why we are asking the Commission and the Council to support this process, and we are also appealing to the Tunisian authorities.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.loc[26].en, scores_df.loc[26].translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our scores, can we improve them? Let's try changing the temperature first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomTempEval(df, system_prompt, temperature = 1):\n",
    "    output_list = []\n",
    "    for en, fr in zip(df.en, df.fr):\n",
    "        message = {}\n",
    "        prompt = f\"Translate the following French text into English: {fr}\"\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": prompt} \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        output_list.append(completion.choices[0].message.content)\n",
    "    new_df = pd.concat([df, pd.Series(output_list, name = 'translation')], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert in language translation, especially between English and French. \n",
    "                    Reply with the only translated text, with no other information.\"\"\"\n",
    "temp_evals_df = CustomTempEval(testing_df, system_prompt, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "      <th>translation</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "      <td>As you may have noticed, the great \"Y2K bug\" d...</td>\n",
       "      <td>0.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It is nevertheless the case that not a penny m...</td>\n",
       "      <td>Il n'en demeure pas moins que pas un centime d...</td>\n",
       "      <td>Nevertheless, not a single cent more than what...</td>\n",
       "      <td>0.841270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Furthermore, although there might be internati...</td>\n",
       "      <td>En outre, quand bien même on parviendrait à un...</td>\n",
       "      <td>Furthermore, even if a global agreement were r...</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(NL) That the Florenz report has caused such a...</td>\n",
       "      <td>(NL) L'ampleur des réactions suscitées par le ...</td>\n",
       "      <td>The scale of the reactions triggered by the Fl...</td>\n",
       "      <td>0.859365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>However, these isolated elements which make se...</td>\n",
       "      <td>Néanmoins, ces paragraphes, marqués au coin du...</td>\n",
       "      <td>Nevertheless, these paragraphs, marked by comm...</td>\n",
       "      <td>0.832945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>It is good for us that we have the power to do...</td>\n",
       "      <td>C'est une bonne chose pour nous que nous ayons...</td>\n",
       "      <td>It is a good thing for us that we have the pow...</td>\n",
       "      <td>0.895085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Mr President, the agreement being promoted wil...</td>\n",
       "      <td>Monsieur le Président, l'accord proposé consti...</td>\n",
       "      <td>Mr. President, the proposed agreement will be ...</td>\n",
       "      <td>0.880732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Moreover, Europe is still the main destination...</td>\n",
       "      <td>De plus l'Europe demeure la principale destina...</td>\n",
       "      <td>Moreover, Europe remains the main tourist dest...</td>\n",
       "      <td>0.877902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Fish escapes from aquaculture premises have be...</td>\n",
       "      <td>On a signalé par le passé la fuite de poissons...</td>\n",
       "      <td>In the past, the escape of fish from aquacultu...</td>\n",
       "      <td>0.882934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Mr President of the Commission, when you enter...</td>\n",
       "      <td>Monsieur le Président de la Commission, lorsqu...</td>\n",
       "      <td>Mr. President of the Commission, when you took...</td>\n",
       "      <td>0.884391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Infectious animal waste is not covered by the ...</td>\n",
       "      <td>Les déchets d'animaux malades n'entrent pas da...</td>\n",
       "      <td>The waste from sick animals is not within the ...</td>\n",
       "      <td>0.860997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>In response to a European Parliament resolutio...</td>\n",
       "      <td>À la suite de la résolution du Parlement europ...</td>\n",
       "      <td>Following the European Parliament's resolution...</td>\n",
       "      <td>0.929447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>The new regime can only win the new local elec...</td>\n",
       "      <td>Seul le nouveau régime est en mesure de rempor...</td>\n",
       "      <td>Only the new regime is able to win the upcomin...</td>\n",
       "      <td>0.847866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>However, we in the Committee on Fisheries cann...</td>\n",
       "      <td>Car, si nous voulons maintenir nos produits su...</td>\n",
       "      <td>Because if we want to keep our products on the...</td>\n",
       "      <td>0.710701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>I also want to remind colleagues that the amen...</td>\n",
       "      <td>Je tiens également à rappeler à mes collègues ...</td>\n",
       "      <td>I would also like to remind my colleagues that...</td>\n",
       "      <td>0.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>We have the blueprints and I urge all of those...</td>\n",
       "      <td>Nous disposons d'un plan et j'exhorte tous ceu...</td>\n",
       "      <td>We have a plan and I urge all those who have t...</td>\n",
       "      <td>0.880979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>I share the minority view of Mr Berthu, a memb...</td>\n",
       "      <td>Je partage le point de vue de la minorité, don...</td>\n",
       "      <td>I share the viewpoint of the minority, as expr...</td>\n",
       "      <td>0.780902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>We can be sure that whatever money we have in ...</td>\n",
       "      <td>Nous pouvons être certains que quel que soit l...</td>\n",
       "      <td>We can be sure that whatever money we have for...</td>\n",
       "      <td>0.908974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Mr President, first of all I should like to co...</td>\n",
       "      <td>Monsieur le Président, tout d' abord, je souha...</td>\n",
       "      <td>Mr. President, first of all, I would like to c...</td>\n",
       "      <td>0.890633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>It is clearly not a perfect text, but we consi...</td>\n",
       "      <td>S'il n'est bien sûr pas parfait, nous estimons...</td>\n",
       "      <td>While it is certainly not perfect, we believe ...</td>\n",
       "      <td>0.853595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Although development cooperation and trade are...</td>\n",
       "      <td>Il est vrai que la coopération au développemen...</td>\n",
       "      <td>It is true that development cooperation and tr...</td>\n",
       "      <td>0.888237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>So far as Miss Kadeer is concerned, obviously ...</td>\n",
       "      <td>En ce qui concerne Mlle Kadeer, manifestement ...</td>\n",
       "      <td>As for Miss Kadeer, I was clearly not aware of...</td>\n",
       "      <td>0.864888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>I would argue that it is more essential, now t...</td>\n",
       "      <td>J'aurais tendance à dire qu'il faut davantage ...</td>\n",
       "      <td>I would tend to say that we need to further gu...</td>\n",
       "      <td>0.898101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Unfortunately, with the exception of support f...</td>\n",
       "      <td>Malheureusement, à l'exception du soutien en f...</td>\n",
       "      <td>Unfortunately, with the exception of support f...</td>\n",
       "      <td>0.910959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>We have witnessed their pain and we have seen ...</td>\n",
       "      <td>Nous avons vu leur douleur et nous avons vu qu...</td>\n",
       "      <td>We have seen their pain and we have seen that ...</td>\n",
       "      <td>0.845342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Mrs Anastassopoulos is in charge of these file...</td>\n",
       "      <td>Madame Anastassopoulos s'occupe de ces dossier...</td>\n",
       "      <td>Mrs. Anastassopoulos is in charge of these fil...</td>\n",
       "      <td>0.854112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>This is why we call upon the Commission and th...</td>\n",
       "      <td>C'est pour cela que nous demandons à la Commis...</td>\n",
       "      <td>That is why we are asking the Commission and t...</td>\n",
       "      <td>0.874155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Finally, if the Feira Council is going to deal...</td>\n",
       "      <td>Enfin, si le Conseil de Feira examine la quest...</td>\n",
       "      <td>Finally, if the Feira Council examines the iss...</td>\n",
       "      <td>0.798944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Madam President, I should like to begin by ass...</td>\n",
       "      <td>. (EN) Madame la Présidente, je voudrais tout ...</td>\n",
       "      <td>Madam President, I would first like to associa...</td>\n",
       "      <td>0.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>It is crucial that European aid has a direct b...</td>\n",
       "      <td>En effet, il est indispensable que l'aide euro...</td>\n",
       "      <td>Indeed, it is essential that European aid dire...</td>\n",
       "      <td>0.884865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>It is the subsidies, tax relief and various fo...</td>\n",
       "      <td>Ce sont les subventions, les dégrèvements fisc...</td>\n",
       "      <td>It is the subsidies, tax breaks, and all kinds...</td>\n",
       "      <td>0.872566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>That is why it was right - and I would like to...</td>\n",
       "      <td>C'est pourquoi il était bon - et je voudrais v...</td>\n",
       "      <td>That is why it was good - and I would like to ...</td>\n",
       "      <td>0.852114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Mr President, ladies and gentlemen, the Europe...</td>\n",
       "      <td>Monsieur le Président, chers collègues, la Com...</td>\n",
       "      <td>Mr. President, dear colleagues, the European C...</td>\n",
       "      <td>0.822960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>Amendments 16 and 19 introduce interesting mat...</td>\n",
       "      <td>On a introduit dans les amendements 16 et 19 d...</td>\n",
       "      <td>Amendments 16 and 19 introduced some very inte...</td>\n",
       "      <td>0.864674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>Interventions of this kind should primarily ta...</td>\n",
       "      <td>De telles interventions doivent d'abord prendr...</td>\n",
       "      <td>Such interventions must first take the form of...</td>\n",
       "      <td>0.895220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>This proposal does not say that all the Member...</td>\n",
       "      <td>Cette proposition ne dit pas que tous les État...</td>\n",
       "      <td>This proposal does not state that all member s...</td>\n",
       "      <td>0.786391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>In the light of an inquiry at the time into th...</td>\n",
       "      <td>Au vu de l'analyse faite à l'époque des conséq...</td>\n",
       "      <td>In light of the analysis conducted at the time...</td>\n",
       "      <td>0.832934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>I call on the Commission to make it crystal cl...</td>\n",
       "      <td>J'invite la Commission à signifier clairement ...</td>\n",
       "      <td>I invite the Commission to make it clear to th...</td>\n",
       "      <td>0.870776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>Firstly, I would like the Commissioner to expl...</td>\n",
       "      <td>Premièrement, je voudrais que le commissaire e...</td>\n",
       "      <td>Firstly, I would like the commissioner to expl...</td>\n",
       "      <td>0.888165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>Another aspect which was close to my heart is ...</td>\n",
       "      <td>Un autre aspect qui me tenait à cur est le fai...</td>\n",
       "      <td>Another aspect that was important to me is the...</td>\n",
       "      <td>0.866490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>I thank Mr Parish for his report and wish his ...</td>\n",
       "      <td>Je remercie M. Parish pour son rapport et je l...</td>\n",
       "      <td>I thank Mr. Parish for his report and wish him...</td>\n",
       "      <td>0.855199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>The MEPs of the Communist Party of Greece will...</td>\n",
       "      <td>En tant que députés européens du Parti communi...</td>\n",
       "      <td>As European Parliament members of the Communis...</td>\n",
       "      <td>0.841301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Progress has been made, as in the case of the ...</td>\n",
       "      <td>Des progrès ont été réalisés, comme celui de l...</td>\n",
       "      <td>Progress has been made, such as the approval i...</td>\n",
       "      <td>0.904739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>We should only concern ourselves with matters ...</td>\n",
       "      <td>Nous devons uniquement nous occuper des questi...</td>\n",
       "      <td>We must only address the issues to which the E...</td>\n",
       "      <td>0.873571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>I do believe that consumer confidence is under...</td>\n",
       "      <td>Je crois vraiment que la confiance des consomm...</td>\n",
       "      <td>I truly believe that consumer confidence is un...</td>\n",
       "      <td>0.894033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>Let us therefore first sort out the nature of ...</td>\n",
       "      <td>Concentrons-nous donc sur les contenus, et nou...</td>\n",
       "      <td>Let's focus on the content, and we can find a ...</td>\n",
       "      <td>0.649844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Now the Commission and the Council have come b...</td>\n",
       "      <td>Et maintenant, voilà que la Commission et le C...</td>\n",
       "      <td>And now, here comes the Commission and the Cou...</td>\n",
       "      <td>0.859952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Mrs Ria, I regret, therefore, your zigzag cour...</td>\n",
       "      <td>Chère Ria, je regrette donc votre progression ...</td>\n",
       "      <td>Dear Ria, I therefore regret your zigzag progr...</td>\n",
       "      <td>0.678676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>The purpose of these urgent topics we are now ...</td>\n",
       "      <td>Les affaires urgentes que nous examinons actue...</td>\n",
       "      <td>The urgent matters we are currently examining ...</td>\n",
       "      <td>0.883769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>The Commission would also decide in future if ...</td>\n",
       "      <td>De plus, la Commission devrait décider des fré...</td>\n",
       "      <td>Furthermore, the Commission should decide on t...</td>\n",
       "      <td>0.829606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                                 en  \\\n",
       "0            0  Although, as you will have seen, the dreaded '...   \n",
       "1            1  It is nevertheless the case that not a penny m...   \n",
       "2            2  Furthermore, although there might be internati...   \n",
       "3            3  (NL) That the Florenz report has caused such a...   \n",
       "4            4  However, these isolated elements which make se...   \n",
       "5            5  It is good for us that we have the power to do...   \n",
       "6            6  Mr President, the agreement being promoted wil...   \n",
       "7            7  Moreover, Europe is still the main destination...   \n",
       "8            8  Fish escapes from aquaculture premises have be...   \n",
       "9            9  Mr President of the Commission, when you enter...   \n",
       "10          10  Infectious animal waste is not covered by the ...   \n",
       "11          11  In response to a European Parliament resolutio...   \n",
       "12          12  The new regime can only win the new local elec...   \n",
       "13          13  However, we in the Committee on Fisheries cann...   \n",
       "14          14  I also want to remind colleagues that the amen...   \n",
       "15          15  We have the blueprints and I urge all of those...   \n",
       "16          16  I share the minority view of Mr Berthu, a memb...   \n",
       "17          17  We can be sure that whatever money we have in ...   \n",
       "18          18  Mr President, first of all I should like to co...   \n",
       "19          19  It is clearly not a perfect text, but we consi...   \n",
       "20          20  Although development cooperation and trade are...   \n",
       "21          21  So far as Miss Kadeer is concerned, obviously ...   \n",
       "22          22  I would argue that it is more essential, now t...   \n",
       "23          23  Unfortunately, with the exception of support f...   \n",
       "24          24  We have witnessed their pain and we have seen ...   \n",
       "25          25  Mrs Anastassopoulos is in charge of these file...   \n",
       "26          26  This is why we call upon the Commission and th...   \n",
       "27          27  Finally, if the Feira Council is going to deal...   \n",
       "28          28  Madam President, I should like to begin by ass...   \n",
       "29          29  It is crucial that European aid has a direct b...   \n",
       "30          30  It is the subsidies, tax relief and various fo...   \n",
       "31          31  That is why it was right - and I would like to...   \n",
       "32          32  Mr President, ladies and gentlemen, the Europe...   \n",
       "33          33  Amendments 16 and 19 introduce interesting mat...   \n",
       "34          34  Interventions of this kind should primarily ta...   \n",
       "35          35  This proposal does not say that all the Member...   \n",
       "36          36  In the light of an inquiry at the time into th...   \n",
       "37          37  I call on the Commission to make it crystal cl...   \n",
       "38          38  Firstly, I would like the Commissioner to expl...   \n",
       "39          39  Another aspect which was close to my heart is ...   \n",
       "40          40  I thank Mr Parish for his report and wish his ...   \n",
       "41          41  The MEPs of the Communist Party of Greece will...   \n",
       "42          42  Progress has been made, as in the case of the ...   \n",
       "43          43  We should only concern ourselves with matters ...   \n",
       "44          44  I do believe that consumer confidence is under...   \n",
       "45          45  Let us therefore first sort out the nature of ...   \n",
       "46          46  Now the Commission and the Council have come b...   \n",
       "47          47  Mrs Ria, I regret, therefore, your zigzag cour...   \n",
       "48          48  The purpose of these urgent topics we are now ...   \n",
       "49          49  The Commission would also decide in future if ...   \n",
       "\n",
       "                                                   fr  \\\n",
       "0   Comme vous avez pu le constater, le grand \"bog...   \n",
       "1   Il n'en demeure pas moins que pas un centime d...   \n",
       "2   En outre, quand bien même on parviendrait à un...   \n",
       "3   (NL) L'ampleur des réactions suscitées par le ...   \n",
       "4   Néanmoins, ces paragraphes, marqués au coin du...   \n",
       "5   C'est une bonne chose pour nous que nous ayons...   \n",
       "6   Monsieur le Président, l'accord proposé consti...   \n",
       "7   De plus l'Europe demeure la principale destina...   \n",
       "8   On a signalé par le passé la fuite de poissons...   \n",
       "9   Monsieur le Président de la Commission, lorsqu...   \n",
       "10  Les déchets d'animaux malades n'entrent pas da...   \n",
       "11  À la suite de la résolution du Parlement europ...   \n",
       "12  Seul le nouveau régime est en mesure de rempor...   \n",
       "13  Car, si nous voulons maintenir nos produits su...   \n",
       "14  Je tiens également à rappeler à mes collègues ...   \n",
       "15  Nous disposons d'un plan et j'exhorte tous ceu...   \n",
       "16  Je partage le point de vue de la minorité, don...   \n",
       "17  Nous pouvons être certains que quel que soit l...   \n",
       "18  Monsieur le Président, tout d' abord, je souha...   \n",
       "19  S'il n'est bien sûr pas parfait, nous estimons...   \n",
       "20  Il est vrai que la coopération au développemen...   \n",
       "21  En ce qui concerne Mlle Kadeer, manifestement ...   \n",
       "22  J'aurais tendance à dire qu'il faut davantage ...   \n",
       "23  Malheureusement, à l'exception du soutien en f...   \n",
       "24  Nous avons vu leur douleur et nous avons vu qu...   \n",
       "25  Madame Anastassopoulos s'occupe de ces dossier...   \n",
       "26  C'est pour cela que nous demandons à la Commis...   \n",
       "27  Enfin, si le Conseil de Feira examine la quest...   \n",
       "28  . (EN) Madame la Présidente, je voudrais tout ...   \n",
       "29  En effet, il est indispensable que l'aide euro...   \n",
       "30  Ce sont les subventions, les dégrèvements fisc...   \n",
       "31  C'est pourquoi il était bon - et je voudrais v...   \n",
       "32  Monsieur le Président, chers collègues, la Com...   \n",
       "33  On a introduit dans les amendements 16 et 19 d...   \n",
       "34  De telles interventions doivent d'abord prendr...   \n",
       "35  Cette proposition ne dit pas que tous les État...   \n",
       "36  Au vu de l'analyse faite à l'époque des conséq...   \n",
       "37  J'invite la Commission à signifier clairement ...   \n",
       "38  Premièrement, je voudrais que le commissaire e...   \n",
       "39  Un autre aspect qui me tenait à cur est le fai...   \n",
       "40  Je remercie M. Parish pour son rapport et je l...   \n",
       "41  En tant que députés européens du Parti communi...   \n",
       "42  Des progrès ont été réalisés, comme celui de l...   \n",
       "43  Nous devons uniquement nous occuper des questi...   \n",
       "44  Je crois vraiment que la confiance des consomm...   \n",
       "45  Concentrons-nous donc sur les contenus, et nou...   \n",
       "46  Et maintenant, voilà que la Commission et le C...   \n",
       "47  Chère Ria, je regrette donc votre progression ...   \n",
       "48  Les affaires urgentes que nous examinons actue...   \n",
       "49  De plus, la Commission devrait décider des fré...   \n",
       "\n",
       "                                          translation    scores  \n",
       "0   As you may have noticed, the great \"Y2K bug\" d...  0.761000  \n",
       "1   Nevertheless, not a single cent more than what...  0.841270  \n",
       "2   Furthermore, even if a global agreement were r...  0.851953  \n",
       "3   The scale of the reactions triggered by the Fl...  0.859365  \n",
       "4   Nevertheless, these paragraphs, marked by comm...  0.832945  \n",
       "5   It is a good thing for us that we have the pow...  0.895085  \n",
       "6   Mr. President, the proposed agreement will be ...  0.880732  \n",
       "7   Moreover, Europe remains the main tourist dest...  0.877902  \n",
       "8   In the past, the escape of fish from aquacultu...  0.882934  \n",
       "9   Mr. President of the Commission, when you took...  0.884391  \n",
       "10  The waste from sick animals is not within the ...  0.860997  \n",
       "11  Following the European Parliament's resolution...  0.929447  \n",
       "12  Only the new regime is able to win the upcomin...  0.847866  \n",
       "13  Because if we want to keep our products on the...  0.710701  \n",
       "14  I would also like to remind my colleagues that...  0.905500  \n",
       "15  We have a plan and I urge all those who have t...  0.880979  \n",
       "16  I share the viewpoint of the minority, as expr...  0.780902  \n",
       "17  We can be sure that whatever money we have for...  0.908974  \n",
       "18  Mr. President, first of all, I would like to c...  0.890633  \n",
       "19  While it is certainly not perfect, we believe ...  0.853595  \n",
       "20  It is true that development cooperation and tr...  0.888237  \n",
       "21  As for Miss Kadeer, I was clearly not aware of...  0.864888  \n",
       "22  I would tend to say that we need to further gu...  0.898101  \n",
       "23  Unfortunately, with the exception of support f...  0.910959  \n",
       "24  We have seen their pain and we have seen that ...  0.845342  \n",
       "25  Mrs. Anastassopoulos is in charge of these fil...  0.854112  \n",
       "26  That is why we are asking the Commission and t...  0.874155  \n",
       "27  Finally, if the Feira Council examines the iss...  0.798944  \n",
       "28  Madam President, I would first like to associa...  0.887200  \n",
       "29  Indeed, it is essential that European aid dire...  0.884865  \n",
       "30  It is the subsidies, tax breaks, and all kinds...  0.872566  \n",
       "31  That is why it was good - and I would like to ...  0.852114  \n",
       "32  Mr. President, dear colleagues, the European C...  0.822960  \n",
       "33  Amendments 16 and 19 introduced some very inte...  0.864674  \n",
       "34  Such interventions must first take the form of...  0.895220  \n",
       "35  This proposal does not state that all member s...  0.786391  \n",
       "36  In light of the analysis conducted at the time...  0.832934  \n",
       "37  I invite the Commission to make it clear to th...  0.870776  \n",
       "38  Firstly, I would like the commissioner to expl...  0.888165  \n",
       "39  Another aspect that was important to me is the...  0.866490  \n",
       "40  I thank Mr. Parish for his report and wish him...  0.855199  \n",
       "41  As European Parliament members of the Communis...  0.841301  \n",
       "42  Progress has been made, such as the approval i...  0.904739  \n",
       "43  We must only address the issues to which the E...  0.873571  \n",
       "44  I truly believe that consumer confidence is un...  0.894033  \n",
       "45  Let's focus on the content, and we can find a ...  0.649844  \n",
       "46  And now, here comes the Commission and the Cou...  0.859952  \n",
       "47  Dear Ria, I therefore regret your zigzag progr...  0.678676  \n",
       "48  The urgent matters we are currently examining ...  0.883769  \n",
       "49  Furthermore, the Commission should decide on t...  0.829606  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scores_df = DataframeCometBatch(temp_evals_df)\n",
    "temp_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.853339056968689, 0.8526142728328705)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scores_df.scores.mean(), scores_df.scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Hopefully!) we see a small increase in performance. This is small, partially due to French being a fairly popular language so is already very optimised. If you would like, please do play around with less common ones - you will see a large increase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is rare that individual calls are made to the API - it is inefficient as the same basic infrastructure to run the code is repeated each time. It is also less cost effective than doing it in bulk (2x). As such we look to batch processing to process our translations. \n",
    "\n",
    "Rather than writing requests in the same form we have been, we need to create a .jsonl file to contain our requests. Here is an example of the format we want for our .jsnol files. One request per line, accessing the completions branch of the API and allowing for differing user/system/assistant messages and parameters. <b>The custom_id needs to be specified uniquely each time.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_id': 'request-2',\n",
       " 'method': 'POST',\n",
       " 'url': '/v1/chat/completions',\n",
       " 'body': {'model': 'gpt-4o-mini',\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'You are an unhelpful assistant.'},\n",
       "   {'role': 'user', 'content': 'Hello world!'}],\n",
       "  'max_tokens': 1000}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "{\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Decide on your system and user prompts and use the function below to generate the jsonl file. Have a look and see if you want to change anything, e.g. temperature, top_p, the amount of few-shot examples. If you would like to, change the functions in llm_utils.py and restart the notebook to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ''\n",
    "few_shot_df = df[-3:] # See if you want to change the number, or where you sample from, or include your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file saved to few_shot_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert in language translation, especially between English and French. Reply with the only translated text, with no other information.\"\"\" # Change this\n",
    "output_file = \"few_shot_batch.jsonl\"\n",
    "    \n",
    "create_jsonl_data_with_custom_id(testing_df, few_shot_df, system_prompt, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, these functions were entirely generated by the free chats available for GPT4o:\n",
    "\n",
    "<em>\"My task is evaluating machine translations, and for that I need to get the translations. as such,I want to create jsonl files for batch processing on the OpenAI API. I have 3 few-shot prompts stored in a dataframe, called few_shot_df, with columns fr for a french sentence and en for english. I also have a dataframe called df which has all the other translations I want to perform, with the same 2 columns of fr and en. My system prompt is \n",
    "\n",
    "system_prompt = \"\"\"You are an expert in language translation, especially between English and French. Reply with the only translated text, with no other information.\"\"\"\n",
    "\n",
    "My prompt structure is prompt = f\"Translate the following French text into English: {df.fr.iloc[idx]}\", where the input of the f-string is the french text I want to translate into english. Write me code to create a jsonl file that calls gpt-4o-mini with the given system prompt, 3-shot prompting and then the given prompt.\"</em>\n",
    "\n",
    "Then:\n",
    "\n",
    "<em>\"Can you refactor these into useful functions that are not too long - nice and modular\"</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask the API to do the batch processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use https://platform.openai.com/docs/guides/batch (can absolutely just copy/paste code) to begin the batch job - put your name or an identifier in the metadata description! Use client.batches.retrieve to take a look at the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# Upload the batch file to OpenAI\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(output_file, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_6797e1f1b4808190af5687b96c1d51ea', completion_window='24h', created_at=1738007025, endpoint='/v1/chat/completions', input_file_id='file-Mo8C6ffvYBdvfZ17dPP5p3', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1738093425, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': \"workshop job - Oli's Batch\"}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"workshop job - Oli's Batch\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET https://api.openai.com/v1/batches/batch_6797e1f1b4808190af5687b96c1d51ea \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6797e1f1b4808190af5687b96c1d51ea', completion_window='24h', created_at=1738007025, endpoint='/v1/chat/completions', input_file_id='file-Mo8C6ffvYBdvfZ17dPP5p3', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1738093425, failed_at=None, finalizing_at=None, in_progress_at=1738007027, metadata={'description': \"workshop job - Oli's Batch\"}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=50))\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.retrieve(\"batch_6797e1f1b4808190af5687b96c1d51ea\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET https://api.openai.com/v1/files/file-HNsiZeKifsjezZX7iXUFbg/content \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "file_response = client.files.content(\"file-\")\n",
    "\n",
    "output_file = 'few_shot_batch_results.jsonl'\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olinewcombe/Documents/GitHub/LLMWorkshop/lesson_notebooks/llm_utils.py:175: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"custom_id\"] = [f\"request-{idx + 1}\" for idx in range(len(df))]\n",
      "/Users/olinewcombe/Documents/GitHub/LLMWorkshop/lesson_notebooks/llm_utils.py:178: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[translation_column] = df[\"custom_id\"].map(translation_map)\n"
     ]
    }
   ],
   "source": [
    "few_shot_results_df = add_translations_to_df(testing_df, output_file, translation_column='translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8110618817806244"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_scores_df = DataframeCometBatch(few_shot_results_df)\n",
    "few_shot_scores_df.scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time, so let's come back to the results later. The process to fine-tune a model is very very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFineTuneJsonl(df, system_prompt, output_file):\n",
    "    content = []\n",
    "    for idx, row in df.iterrows():\n",
    "        messages = [create_system_message(system_prompt), \n",
    "        {\"role\" : \"user\", \"content\" : f\"Translate the following French text into English: {row.fr}\"},  \n",
    "        {\"role\" : \"assistant\", \"content\" : f\"{row.en}\"}]\n",
    "        content.append({\"messages\": messages})\n",
    "    save_to_jsonl(content, output_file)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_df = df[-100:]\n",
    "output_file = \"finetune_batch.jsonl\"\n",
    "system_prompt = \"\"\"You are an expert in language translation, especially between English and French. Reply with the only translated text, with no other information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file saved to finetune_batch.jsonl\n"
     ]
    }
   ],
   "source": [
    "CreateFineTuneJsonl(finetune_df, system_prompt, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-KzegkpfUWB9Lb58dyvF78K', bytes=64369, created_at=1738009502, filename='finetune_batch.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(\n",
    "  file=open(output_file, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/fine_tuning/jobs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-2C2pXoYsYbf3Ll0yf1w6a6s4', created_at=1738009567, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-Dm9H5NobPutslK2NG4vsyAm6', result_files=[], seed=1681402647, status='validating_files', trained_tokens=None, training_file='file-KzegkpfUWB9Lb58dyvF78K', validation_file=None, estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto')), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-KzegkpfUWB9Lb58dyvF78K\",\n",
    "    model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: GET https://api.openai.com/v1/fine_tuning/jobs/ftjob-2C2pXoYsYbf3Ll0yf1w6a6s4 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ft:gpt-4o-mini-2024-07-18:oliprojects::AuQ6VO4e'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(\"ftjob-2C2pXoYsYbf3Ll0yf1w6a6s4\").fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert in language translation, especially translating French into English. \n",
    "                    Reply with the only translated text, with no other information.\"\"\"\n",
    "finetune_evals_df = BaselineEval(testing_df, system_prompt, model = 'ft:gpt-4o-mini-2024-07-18:oliprojects::AuQ6VO4e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "finetune_scores_df = DataframeCometBatch(finetune_evals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8390222835540772"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_scores_df.scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERES ONE I FINETUNED EARLIER!! - whilst we wait I can talk about UKHSA RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
